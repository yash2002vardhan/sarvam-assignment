{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, max_vocab=100000):\n",
    "    \"\"\"\n",
    "    Loads FastText word embeddings from a file.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the FastText embeddings file\n",
    "        max_vocab (int): Maximum number of word embeddings to load\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping words to their vector embeddings\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with io.open(file_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)  # Skip header\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx >= max_vocab:\n",
    "                break\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            word = tokens[0]\n",
    "            vector = np.array(tokens[1:], dtype=np.float32)\n",
    "            embeddings[word] = vector\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bilingual_lexicon(file_path, en_embeddings, hi_embeddings):\n",
    "    \"\"\"\n",
    "    Loads a bilingual lexicon and filters word pairs based on available embeddings.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the bilingual lexicon file\n",
    "        en_embeddings (dict): English word embeddings dictionary\n",
    "        hi_embeddings (dict): Hindi word embeddings dictionary\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Lists of aligned English and Hindi words that exist in both embedding spaces\n",
    "    \"\"\"\n",
    "    en_words = []\n",
    "    hi_words = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) > 2:\n",
    "                continue\n",
    "            en_word, hi_word = line.strip().split()\n",
    "            if en_word in en_embeddings and hi_word in hi_embeddings:\n",
    "                en_words.append(en_word)\n",
    "                hi_words.append(hi_word)\n",
    "    return en_words, hi_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_alignment(en_embeddings, hi_embeddings, en_lexicon, hi_lexicon):\n",
    "    \"\"\"\n",
    "    Performs Procrustes alignment to find the optimal linear transformation between two embedding spaces.\n",
    "    \n",
    "    Args:\n",
    "        en_embeddings (dict): English word embeddings\n",
    "        hi_embeddings (dict): Hindi word embeddings  \n",
    "        en_lexicon (list): English words from bilingual lexicon\n",
    "        hi_lexicon (list): Hindi words from bilingual lexicon\n",
    "        \n",
    "    Returns:\n",
    "        ndarray: The learned transformation matrix W\n",
    "    \"\"\"\n",
    "    en_matrix = np.array([en_embeddings[word] for word in en_lexicon])\n",
    "    hi_matrix = np.array([hi_embeddings[word] for word in hi_lexicon])\n",
    "\n",
    "    # Normalize the embeddings\n",
    "    en_matrix = en_matrix / np.linalg.norm(en_matrix, axis=1)[:, np.newaxis]\n",
    "    hi_matrix = hi_matrix / np.linalg.norm(hi_matrix, axis=1)[:, np.newaxis]\n",
    "\n",
    "    # Calculate transformation matrix using SVD\n",
    "    U, _, Vt = np.linalg.svd(np.dot(en_matrix.T, hi_matrix))\n",
    "    W = np.dot(U, Vt)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mapping(en_embeddings, W):\n",
    "    \"\"\"\n",
    "    Applies the learned transformation matrix to English embeddings.\n",
    "    \n",
    "    Args:\n",
    "        en_embeddings (dict): English word embeddings\n",
    "        W (ndarray): Transformation matrix from Procrustes alignment\n",
    "        \n",
    "    Returns:\n",
    "        dict: Transformed English embeddings aligned to Hindi embedding space\n",
    "    \"\"\"\n",
    "    aligned_en_embeddings = {}\n",
    "    for word, vector in en_embeddings.items():\n",
    "        aligned_en_embeddings[word] = np.dot(vector, W)\n",
    "    return aligned_en_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(en_word, aligned_en_embeddings, hi_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Translates an English word to Hindi by finding nearest neighbors in the aligned embedding space.\n",
    "    \n",
    "    Args:\n",
    "        en_word (str): English word to translate\n",
    "        aligned_en_embeddings (dict): Aligned English embeddings\n",
    "        hi_embeddings (dict): Hindi embeddings\n",
    "        k (int): Number of translation candidates to return\n",
    "        \n",
    "    Returns:\n",
    "        list: Top k Hindi translation candidates\n",
    "    \"\"\"\n",
    "    if en_word not in aligned_en_embeddings:\n",
    "        return []\n",
    "    en_vector = aligned_en_embeddings[en_word]\n",
    "    similarities = {}\n",
    "    for hi_word, hi_vector in hi_embeddings.items():\n",
    "        similarities[hi_word] = np.dot(en_vector, hi_vector) / (np.linalg.norm(en_vector) * np.linalg.norm(hi_vector))\n",
    "    sorted_translations = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [word for word, _ in sorted_translations[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translation(aligned_en_embeddings, hi_embeddings, test_lexicon_path, k=5):\n",
    "    \"\"\"\n",
    "    Evaluates translation accuracy using Precision@k metrics.\n",
    "    \n",
    "    Args:\n",
    "        aligned_en_embeddings (dict): Aligned English embeddings\n",
    "        hi_embeddings (dict): Hindi embeddings\n",
    "        test_lexicon_path (str): Path to test bilingual lexicon\n",
    "        k (int): Maximum number of translation candidates to consider\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Precision@1 and Precision@5 scores\n",
    "    \"\"\"\n",
    "    test_en_words, test_hi_words = load_bilingual_lexicon(test_lexicon_path, en_embeddings, hi_embeddings)\n",
    "    precision_at_1 = 0\n",
    "    precision_at_5 = 0\n",
    "    for en_word, hi_word in zip(test_en_words, test_hi_words):\n",
    "        translations = translate_word(en_word, aligned_en_embeddings, hi_embeddings, k)\n",
    "        if hi_word in translations[:1]:\n",
    "            precision_at_1 += 1\n",
    "        if hi_word in translations:\n",
    "            precision_at_5 += 1\n",
    "    precision_at_1 /= len(test_en_words)\n",
    "    precision_at_5 /= len(test_en_words)\n",
    "    return precision_at_1, precision_at_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(word1, word2, embeddings1, embeddings2, W=None):\n",
    "    \"\"\"\n",
    "    Computes cosine similarity between two words in their respective embedding spaces.\n",
    "    \n",
    "    Args:\n",
    "        word1 (str): First word\n",
    "        word2 (str): Second word\n",
    "        embeddings1 (dict): First embedding space\n",
    "        embeddings2 (dict): Second embedding space\n",
    "        W (ndarray, optional): Transformation matrix to align embeddings1 to embeddings2\n",
    "        \n",
    "    Returns:\n",
    "        float: Cosine similarity between the word vectors\n",
    "    \"\"\"\n",
    "    if W is not None:\n",
    "      embeddings1[word1] = np.dot(embeddings1[word1], W)\n",
    "    vec1 = embeddings1[word1]\n",
    "    vec2 = embeddings2[word2]\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ablation_study(en_embeddings, hi_embeddings, lexicon_path, test_lexicon_path, sizes=[5000, 10000, 20000]):\n",
    "    \"\"\"\n",
    "    Conducts an ablation study to analyze performance with different lexicon sizes.\n",
    "    \n",
    "    Args:\n",
    "        en_embeddings (dict): English embeddings\n",
    "        hi_embeddings (dict): Hindi embeddings\n",
    "        lexicon_path (str): Path to training bilingual lexicon\n",
    "        test_lexicon_path (str): Path to test bilingual lexicon\n",
    "        sizes (list): Different lexicon sizes to test\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary mapping lexicon sizes to (Precision@1, Precision@5) scores\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for size in sizes:\n",
    "        en_lexicon, hi_lexicon = load_bilingual_lexicon(lexicon_path, en_embeddings, hi_embeddings)\n",
    "        en_lexicon = en_lexicon[:size]\n",
    "        hi_lexicon = hi_lexicon[:size]\n",
    "        W = procrustes_alignment(en_embeddings, hi_embeddings, en_lexicon, hi_lexicon)\n",
    "        aligned_en_embeddings = apply_mapping(en_embeddings, W)\n",
    "        precision_1, precision_5 = evaluate_translation(aligned_en_embeddings, hi_embeddings, test_lexicon_path, 5)\n",
    "        results[size] = (precision_1, precision_5)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load word embeddings for English and Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embeddings = load_embeddings(\"wiki.en.vec\", max_vocab=100000) #Replace with your path\n",
    "hi_embeddings = load_embeddings(\"wiki.hi.vec\", max_vocab=100000) #Replace with your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(len(en_embeddings))\n",
    "print(len(hi_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load bilingual lexicon and perform alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_lexicon, hi_lexicon = load_bilingual_lexicon(\"en-hi.txt\", en_embeddings, hi_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = procrustes_alignment(en_embeddings, hi_embeddings, en_lexicon, hi_lexicon)\n",
    "aligned_en_embeddings = apply_mapping(en_embeddings, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate translation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.5934244791666666\n",
      "Precision@5: 0.7805989583333334\n"
     ]
    }
   ],
   "source": [
    "precision_1, precision_5 = evaluate_translation(aligned_en_embeddings, hi_embeddings, \"en-hi.txt\", 5)\n",
    "print(f\"Precision@1: {precision_1}\")\n",
    "print(f\"Precision@5: {precision_5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with example word pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.3127928376197815\n"
     ]
    }
   ],
   "source": [
    "sim = compute_cosine_similarity(\"hello\",\"नमस्ते\", en_embeddings, hi_embeddings, W)\n",
    "print(f\"Cosine similarity: {sim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform ablation study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ablation Study Results:\n",
      "Size: 5000, Precision@1: 0.5940755208333334, Precision@5: 0.7835286458333334\n",
      "Size: 10000, Precision@1: 0.5940755208333334, Precision@5: 0.7835286458333334\n",
      "Size: 20000, Precision@1: 0.5940755208333334, Precision@5: 0.7835286458333334\n"
     ]
    }
   ],
   "source": [
    "ablation_results = ablation_study(en_embeddings, hi_embeddings, \"en-hi.txt\", \"en-hi.txt\")\n",
    "print(\"Ablation Study Results:\")\n",
    "for size, (p1, p5) in ablation_results.items():\n",
    "    print(f\"Size: {size}, Precision@1: {p1}, Precision@5: {p5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
