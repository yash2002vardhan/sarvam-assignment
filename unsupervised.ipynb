{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set device to MPS (Mac), CUDA (GPU) or CPU based on availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, max_vocab=100000):\n",
    "    \"\"\"\n",
    "    Load pre-trained word embeddings from a file in word2vec format.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to embeddings file\n",
    "        max_vocab (int): Maximum vocabulary size to load\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (embeddings tensor, word to ID dict, ID to word dict)\n",
    "    \"\"\"\n",
    "    word2id, id2word, vectors = {}, {}, []\n",
    "    with open(file_path, 'r', encoding='utf-8', newline='\\n') as f:\n",
    "        next(f)  # Skip header\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx >= max_vocab:\n",
    "                break\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) < 10:\n",
    "                continue\n",
    "            try:\n",
    "                word, vec = tokens[0], np.array(tokens[1:], dtype=np.float32)\n",
    "            except:\n",
    "                continue\n",
    "            if vec.shape[0] != 300:\n",
    "                continue\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            vectors.append(vec)\n",
    "    return torch.tensor(np.vstack(vectors), dtype=torch.float32).to(device), word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(emb):\n",
    "    \"\"\"\n",
    "    Center and L2-normalize embeddings.\n",
    "    \n",
    "    Args:\n",
    "        emb (torch.Tensor): Input embeddings\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Normalized embeddings\n",
    "    \"\"\"\n",
    "    emb = emb - emb.mean(dim=0, keepdim=True)\n",
    "    emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator model that learns a linear mapping between source and target embedding spaces.\n",
    "    Uses an orthogonal matrix initialization and regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): Dimension of the word embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(embedding_dim, embedding_dim))\n",
    "        nn.init.orthogonal_(self.W)\n",
    "    def forward(self, x):\n",
    "        \"\"\"Transform source embeddings to target space\"\"\"\n",
    "        return x @ self.W\n",
    "    def orthogonalize(self, beta=0.01):\n",
    "        \"\"\"Apply soft orthogonality constraint to mapping matrix\"\"\"\n",
    "        with torch.no_grad():\n",
    "            W = self.W.data\n",
    "            self.W.data = (1 + beta) * W - beta * W @ W.T @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator model that tries to distinguish between real target embeddings\n",
    "    and fake ones produced by the generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Dimension of the input embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(4096, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        \"\"\"Classify input embeddings as real/fake\"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csls_similarity(src_vecs, tgt_vecs, k=10):\n",
    "    \"\"\"\n",
    "    Compute Cross-domain Similarity Local Scaling (CSLS) between source and target vectors.\n",
    "    This reduces the hubness problem in high-dimensional spaces.\n",
    "    \n",
    "    Args:\n",
    "        src_vecs (torch.Tensor): Source vectors\n",
    "        tgt_vecs (torch.Tensor): Target vectors\n",
    "        k (int): Number of nearest neighbors to consider\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: CSLS similarity scores\n",
    "    \"\"\"\n",
    "    src_vecs = F.normalize(src_vecs, dim=1, p=2)\n",
    "    tgt_vecs = F.normalize(tgt_vecs, dim=1, p=2)\n",
    "    sim_matrix = torch.matmul(src_vecs, tgt_vecs.T)\n",
    "    k = min(k, src_vecs.size(0), tgt_vecs.size(0))\n",
    "    src_knn_sim = torch.topk(sim_matrix, k, dim=1, largest=True).values.mean(dim=1)\n",
    "    tgt_knn_sim = torch.topk(sim_matrix, k, dim=0, largest=True).values.mean(dim=0)\n",
    "    csls_scores = 2 * sim_matrix - src_knn_sim[:, None] - tgt_knn_sim[None, :]\n",
    "    return csls_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, src_emb, tgt_emb, epochs=10, batch_size=128, lr=0.001, save_path=\"best_model.pth\", patience=5):\n",
    "    \"\"\"\n",
    "    Train the adversarial model.\n",
    "    \n",
    "    Args:\n",
    "        generator (Generator): Generator model\n",
    "        discriminator (Discriminator): Discriminator model\n",
    "        src_emb (torch.Tensor): Source embeddings\n",
    "        tgt_emb (torch.Tensor): Target embeddings\n",
    "        epochs (int): Number of training epochs\n",
    "        batch_size (int): Batch size\n",
    "        lr (float): Learning rate\n",
    "        save_path (str): Path to save best model\n",
    "        patience (int): Early stopping patience\n",
    "    \"\"\"\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    dis_opt = torch.optim.Adam(discriminator.parameters(), lr=lr * 2)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve_epochs = 0\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss_gen = 0\n",
    "        epoch_loss_dis = 0\n",
    "        for i in range(0, min(len(src_emb), len(tgt_emb)), batch_size):\n",
    "            src_batch = src_emb[i:i+batch_size]\n",
    "            tgt_batch = tgt_emb[i:i+batch_size]\n",
    "\n",
    "            # ----- Train Discriminator -----\n",
    "            for _ in range(5):\n",
    "                gen_emb = generator(src_batch).detach()\n",
    "                real = tgt_batch\n",
    "                fake = gen_emb\n",
    "                real_labels = torch.ones(real.size(0), 1).to(device)\n",
    "                fake_labels = torch.zeros(fake.size(0), 1).to(device)\n",
    "                dis_real = discriminator(real)\n",
    "                dis_fake = discriminator(fake)\n",
    "                loss_real = bce_loss(dis_real, real_labels)\n",
    "                loss_fake = bce_loss(dis_fake, fake_labels)\n",
    "                loss_dis = (loss_real + loss_fake) / 2\n",
    "                dis_opt.zero_grad()\n",
    "                loss_dis.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "            # ----- Train Generator -----\n",
    "            gen_emb = generator(src_batch)\n",
    "            pred = discriminator(gen_emb)\n",
    "            loss_gen = bce_loss(pred, torch.ones_like(pred))\n",
    "            gen_opt.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            gen_opt.step()\n",
    "            generator.orthogonalize()\n",
    "            epoch_loss_gen += loss_gen.item()\n",
    "            epoch_loss_dis += loss_dis.item()\n",
    "        avg_loss_gen = epoch_loss_gen / (len(src_emb) // batch_size)\n",
    "        avg_loss_dis = epoch_loss_dis / (len(src_emb) // batch_size)\n",
    "        print(f\"Epoch {epoch+1}: Gen Loss: {avg_loss_gen:.4f}, Dis Loss: {avg_loss_dis:.4f}\")\n",
    "\n",
    "        # ----- Early stopping & saving -----\n",
    "        if avg_loss_gen < best_loss:\n",
    "            best_loss = avg_loss_gen\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"New best loss! Saving model to {save_path}\")\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1} due to no improvement for {patience} epochs.\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(word, src_emb, tgt_emb, src_w2id, tgt_id2w, generator, top_k=1):\n",
    "    \"\"\"\n",
    "    Translate a single word using the trained generator.\n",
    "    \n",
    "    Args:\n",
    "        word (str): Word to translate\n",
    "        src_emb (torch.Tensor): Source embeddings\n",
    "        tgt_emb (torch.Tensor): Target embeddings\n",
    "        src_w2id (dict): Source word to ID mapping\n",
    "        tgt_id2w (dict): Target ID to word mapping\n",
    "        generator (Generator): Trained generator model\n",
    "        top_k (int): Number of translation candidates to return\n",
    "        \n",
    "    Returns:\n",
    "        list: Top k translation candidates\n",
    "    \"\"\"\n",
    "    idx = src_w2id.get(word, None)\n",
    "    if idx is None:\n",
    "        return [\"<UNK>\"]\n",
    "    src_vec = src_emb[idx].unsqueeze(0)\n",
    "    projected = generator(src_vec)\n",
    "    csls_scores = compute_csls_similarity(projected, tgt_emb, k=10)\n",
    "    \n",
    "    # Get the top k indices and ensure they're within bounds\n",
    "    max_idx = len(tgt_id2w) - 1\n",
    "    best_match_ids = csls_scores[0].topk(min(top_k, max_idx + 1)).indices.tolist()\n",
    "    valid_indices = [i for i in best_match_ids if 0 <= i <= max_idx]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        return [\"<UNK>\"]\n",
    "    \n",
    "    try:\n",
    "        return [tgt_id2w[i] for i in valid_indices]\n",
    "    except KeyError:\n",
    "        return [\"<UNK>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test word pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words, hi_words  = [], []\n",
    "with open(\"en-hi-test.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        en_word, hi_word = parts\n",
    "        en_words.append(en_word)\n",
    "        hi_words.append(hi_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Gen Loss: 7.7177, Dis Loss: 0.0311\n",
      "New best loss! Saving model to best_model.pth\n",
      "Epoch 2: Gen Loss: 9.1424, Dis Loss: 0.0197\n",
      "Epoch 3: Gen Loss: 9.7748, Dis Loss: 0.0188\n",
      "Epoch 4: Gen Loss: 10.2146, Dis Loss: 0.0159\n",
      "Epoch 5: Gen Loss: 10.5365, Dis Loss: 0.0153\n",
      "Epoch 6: Gen Loss: 11.1851, Dis Loss: 0.0111\n",
      "Early stopping at epoch 6 due to no improvement for 5 epochs.\n",
      "Loading best saved model...\n",
      "Final Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load pre-trained embeddings\n",
    "    en_vecs, en_w2id, en_id2w = load_embeddings(\"wiki.en.vec\", max_vocab=100000)\n",
    "    hi_vecs, hi_w2id, hi_id2w = load_embeddings(\"wiki.hi.vec\", max_vocab=100000)\n",
    "\n",
    "    # Initialize models\n",
    "    generator = Generator(300).to(device)\n",
    "    discriminator = Discriminator(300).to(device)\n",
    "\n",
    "    # Train models\n",
    "    train(generator, discriminator, en_vecs, hi_vecs, epochs=50, batch_size=32, lr=0.0001, save_path=\"best_model.pth\", patience=5)\n",
    "\n",
    "    # Load best model\n",
    "    print(\"Loading best saved model...\")\n",
    "    checkpoint = torch.load(\"best_model.pth\", map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "\n",
    "    # Evaluate\n",
    "    translated_words = []\n",
    "    for word in en_words:\n",
    "        translated = translate_word(word, en_vecs, hi_vecs, en_w2id, hi_id2w, generator)\n",
    "        translated_words.append(translated[0])\n",
    "    correct = 0\n",
    "    for pred, true in zip(translated_words, hi_words):\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "    score = correct / len(translated_words)\n",
    "    print(f\"Final Accuracy: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
