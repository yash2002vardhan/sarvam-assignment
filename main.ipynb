{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load pre-trained embeddings\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "# fasttext.util.download_model('hi', if_exists='ignore')  # Hindi\n",
    "\n",
    "# Load models\n",
    "en_model = fasttext.load_model('cc.en.300.bin')\n",
    "hi_model = fasttext.load_model('cc.hi.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(model, num_words=100000):\n",
    "    return {word: model[word] for word in model.get_words()[:num_words]}\n",
    "\n",
    "en_vocab = get_top_words(en_model)\n",
    "hi_vocab = get_top_words(hi_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MUSE bilingual dictionary\n",
    "def load_muse_dict(filepath):\n",
    "    word_pairs = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split('\\t')  # Split on tab since that's the delimiter in the file\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Skipping line {i+1} - incorrect format\")\n",
    "                continue\n",
    "            en_word, hi_word = parts\n",
    "            if en_word in en_vocab and hi_word in hi_vocab:\n",
    "                word_pairs.append((en_word, hi_word))\n",
    "    return word_pairs\n",
    "\n",
    "# Example: Assume we have the MUSE dataset for English-Hindi\n",
    "muse_dict_path = 'en-hi.txt'\n",
    "bilingual_dict = load_muse_dict(muse_dict_path)\n",
    "\n",
    "# Convert to NumPy arrays for alignment\n",
    "X = np.array([en_vocab[pair[0]] for pair in bilingual_dict])\n",
    "Y = np.array([hi_vocab[pair[1]] for pair in bilingual_dict])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_alignment(X, Y):\n",
    "    # Compute the cross-covariance matrix\n",
    "    M = Y.T @ X\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, _, Vt = np.linalg.svd(M)\n",
    "    \n",
    "    # Compute optimal transformation\n",
    "    W = U @ Vt\n",
    "    return W\n",
    "\n",
    "W = procrustes_alignment(X, Y)\n",
    "\n",
    "# Apply transformation to English embeddings\n",
    "X_aligned = X @ W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.0000\n",
      "Precision@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert Hindi embeddings to a search space\n",
    "hi_matrix = np.array([hi_vocab[word] for word in hi_vocab.keys()])\n",
    "hi_words = list(hi_vocab.keys())\n",
    "\n",
    "# Compute cosine similarity\n",
    "def get_nearest_neighbors(embedded_word, k=5):\n",
    "    similarities = cosine_similarity([embedded_word], hi_matrix)[0]\n",
    "    nearest_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return [hi_words[idx] for idx in nearest_indices]\n",
    "\n",
    "# Evaluate Precision@1 and Precision@5\n",
    "def evaluate_translation(X_aligned, bilingual_dict):\n",
    "    correct_at_1, correct_at_5 = 0, 0\n",
    "    total = len(bilingual_dict)\n",
    "    \n",
    "    for i, (en_word, hi_word) in enumerate(bilingual_dict):\n",
    "        nearest_neighbors = get_nearest_neighbors(X_aligned[i], k=5)\n",
    "        \n",
    "        if hi_word == nearest_neighbors[0]:  # Precision@1\n",
    "            correct_at_1 += 1\n",
    "        \n",
    "        if hi_word in nearest_neighbors:  # Precision@5\n",
    "            correct_at_5 += 1\n",
    "    \n",
    "    precision_at_1 = correct_at_1 / total\n",
    "    precision_at_5 = correct_at_5 / total\n",
    "    return precision_at_1, precision_at_5\n",
    "\n",
    "precision_1, precision_5 = evaluate_translation(X_aligned, bilingual_dict)\n",
    "\n",
    "print(f'Precision@1: {precision_1:.4f}')\n",
    "print(f'Precision@5: {precision_5:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity(king, राजा) = -0.0519\n",
      "Cosine Similarity(queen, रानी) = -0.0623\n",
      "Cosine Similarity(apple, सेब) = 0.1291\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(word1, word2):\n",
    "    return cosine_similarity([en_vocab[word1]], [hi_vocab[word2]])[0, 0]\n",
    "\n",
    "# Example word pairs\n",
    "example_pairs = [('king', 'राजा'), ('queen', 'रानी'), ('apple', 'सेब')]\n",
    "\n",
    "for en_word, hi_word in example_pairs:\n",
    "    if en_word in en_vocab and hi_word in hi_vocab:\n",
    "        sim = cosine_sim(en_word, hi_word)\n",
    "        print(f\"Cosine Similarity({en_word}, {hi_word}) = {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilingual Lexicon Size: 5000, Precision@1: 0.0000, Precision@5: 0.0000\n",
      "Bilingual Lexicon Size: 10000, Precision@1: 0.0000, Precision@5: 0.0000\n",
      "Bilingual Lexicon Size: 20000, Precision@1: 0.0000, Precision@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for size in [5000, 10000, 20000]:\n",
    "    subset_dict = bilingual_dict[:size]\n",
    "    X_sub = np.array([en_vocab[pair[0]] for pair in subset_dict])\n",
    "    Y_sub = np.array([hi_vocab[pair[1]] for pair in subset_dict])\n",
    "    \n",
    "    W_sub = procrustes_alignment(X_sub, Y_sub)\n",
    "    X_aligned_sub = X_sub @ W_sub\n",
    "    p1, p5 = evaluate_translation(X_aligned_sub, subset_dict)\n",
    "    \n",
    "    print(f\"Bilingual Lexicon Size: {size}, Precision@1: {p1:.4f}, Precision@5: {p5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
