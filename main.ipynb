{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load pre-trained embeddings\n",
    "# fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "# fasttext.util.download_model('hi', if_exists='ignore')  # Hindi\n",
    "\n",
    "# Load models\n",
    "en_model = fasttext.load_model('cc.en.300.bin')\n",
    "hi_model = fasttext.load_model('cc.hi.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_words(model, num_words=100000):\n",
    "    return {word: model[word] for word in model.get_words()[:num_words]}\n",
    "\n",
    "en_vocab = get_top_words(en_model)\n",
    "hi_vocab = get_top_words(hi_model)\n",
    "\n",
    "# en_vocab  = {word: en_model[word] for word in en_model.get_words()}\n",
    "# hi_vocab  = {word: hi_model[word] for word in hi_model.get_words()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"en-hi.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    words = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38221\n"
     ]
    }
   ],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38221\n"
     ]
    }
   ],
   "source": [
    "en_words = []\n",
    "for i in words:\n",
    "    parts = i.strip().split(\"\\t\")\n",
    "    if len(parts) != 2:\n",
    "        print(f\"Skipping line {i} - incorrect format\")\n",
    "        continue\n",
    "    en_word, hi_word = parts\n",
    "    en_words.append(en_word)\n",
    "\n",
    "print(len(en_words))\n",
    "\n",
    "en_words_set = list(set(en_words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31719\n"
     ]
    }
   ],
   "source": [
    "print(len(en_words_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31717\n"
     ]
    }
   ],
   "source": [
    "with open(\"hi_words.txt\", \"r\", encoding = \"latin\") as f:\n",
    "    hi_words = [line.strip() for line in f]\n",
    "\n",
    "print(len(hi_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "bilingual_dict = []\n",
    "for i, j in zip(en_words_set, hi_words):\n",
    "    if i in en_vocab and j in hi_vocab:\n",
    "        bilingual_dict.append((i, j))\n",
    "\n",
    "print(len(bilingual_dict))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tenor', 'ISSN')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilingual_dict[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hindi word value: HIV\n",
      "Hindi word value: ovulation\n",
      "Hindi word value: Paypal\n",
      "Hindi word value: Asus\n",
      "Hindi word value: wraps\n",
      "Hindi word value: USB\n",
      "Hindi word value: USD\n",
      "Hindi word value: nowrap\n",
      "Hindi word value: XVI\n",
      "Hindi word value: testdebutagainst\n",
      "Hindi word value: IBS\n",
      "Hindi word value: Flipkart\n",
      "Hindi word value: IETF\n",
      "Hindi word value: IUPAC\n",
      "Hindi word value: thermodynamic\n",
      "Hindi word value: jQuery\n",
      "Hindi word value: RTECS\n",
      "Hindi word value: KFC\n",
      "Hindi word value: usercategory\n",
      "Hindi word value: gsub\n",
      "Hindi word value: trevally\n",
      "Hindi word value: XXXX\n",
      "Hindi word value: PST\n",
      "Hindi word value: arya\n",
      "Hindi word value: UMTS\n",
      "Hindi word value: GFDL\n",
      "Hindi word value: CPU\n",
      "Hindi word value: ISSN\n",
      "Hindi word value: HSDPA\n",
      "Hindi word value: WWWF\n",
      "Hindi word value: IPKF\n",
      "Hindi word value: GPS\n",
      "Hindi word value: glottorefname\n",
      "Hindi word value: KHAN\n",
      "Hindi word value: Yandex\n",
      "Hindi word value: ROM\n",
      "Hindi word value: Cervantes\n",
      "Hindi word value: MOSFET\n",
      "Hindi word value: MediaWiki\n",
      "Hindi word value: KKK\n",
      "Hindi word value: croaker\n",
      "Hindi word value: Crayola\n",
      "Hindi word value: YouTube\n",
      "Hindi word value: Teatro\n",
      "Hindi word value: XXV\n",
      "Hindi word value: HTTP\n",
      "Hindi word value: yaa\n",
      "Hindi word value: Suffolk\n",
      "Hindi word value: removal\n",
      "Hindi word value: isnotempty\n",
      "Hindi word value: Lalitpur\n",
      "Hindi word value: XXVI\n",
      "Hindi word value: Lenovo\n",
      "Hindi word value: CDC\n"
     ]
    }
   ],
   "source": [
    "# Load MUSE bilingual dictionary\n",
    "def load_muse_dict(filepath):\n",
    "    word_pairs = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            parts = line.strip().split('\\t')  # Split on tab since that's the delimiter in the file\n",
    "            if len(parts) != 2:\n",
    "                print(f\"Skipping line {i+1} - incorrect format\")\n",
    "                continue\n",
    "            en_word, hi_word = parts\n",
    "            if hi_word in hi_vocab and en_word in en_vocab:\n",
    "                for key, value in hi_vocab.items():\n",
    "                    if key == hi_word:\n",
    "                        print(\"Hindi word value:\", key)\n",
    "                word_pairs.append((en_word, hi_word))\n",
    "    return word_pairs\n",
    "\n",
    "# Example: Assume we have the MUSE dataset for English-Hindi\n",
    "muse_dict_path = 'en-hi-personal.txt'\n",
    "bilingual_dict = load_muse_dict(muse_dict_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15627\n"
     ]
    }
   ],
   "source": [
    "print(len(bilingual_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays for alignment\n",
    "X = np.array([en_vocab[pair[0]] for pair in bilingual_dict])\n",
    "Y = np.array([hi_vocab[pair[1]] for pair in bilingual_dict])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_alignment(X, Y):\n",
    "    # Compute the cross-covariance matrix\n",
    "    M = Y.T @ X\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, _, Vt = np.linalg.svd(M)\n",
    "    \n",
    "    # Compute optimal transformation\n",
    "    W = U @ Vt\n",
    "    return W\n",
    "\n",
    "W = procrustes_alignment(X, Y)\n",
    "\n",
    "# Apply transformation to English embeddings\n",
    "X_aligned = X @ W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@1: 0.0000\n",
      "Precision@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Convert Hindi embeddings to a search space\n",
    "hi_matrix = np.array([hi_vocab[word] for word in hi_vocab.keys()])\n",
    "hi_words = list(hi_vocab.keys())\n",
    "\n",
    "# Compute cosine similarity\n",
    "def get_nearest_neighbors(embedded_word, k=5):\n",
    "    similarities = cosine_similarity([embedded_word], hi_matrix)[0]\n",
    "    nearest_indices = np.argsort(similarities)[::-1][:k]\n",
    "    return [hi_words[idx] for idx in nearest_indices]\n",
    "\n",
    "# Evaluate Precision@1 and Precision@5\n",
    "def evaluate_translation(X_aligned, bilingual_dict):\n",
    "    correct_at_1, correct_at_5 = 0, 0\n",
    "    total = len(bilingual_dict)\n",
    "    \n",
    "    for i, (en_word, hi_word) in enumerate(bilingual_dict):\n",
    "        nearest_neighbors = get_nearest_neighbors(X_aligned[i], k=5)\n",
    "        \n",
    "        if hi_word == nearest_neighbors[0]:  # Precision@1\n",
    "            correct_at_1 += 1\n",
    "        \n",
    "        if hi_word in nearest_neighbors:  # Precision@5\n",
    "            correct_at_5 += 1\n",
    "    \n",
    "    precision_at_1 = correct_at_1 / total\n",
    "    precision_at_5 = correct_at_5 / total\n",
    "    return precision_at_1, precision_at_5\n",
    "\n",
    "precision_1, precision_5 = evaluate_translation(X_aligned, bilingual_dict)\n",
    "\n",
    "print(f'Precision@1: {precision_1:.4f}')\n",
    "print(f'Precision@5: {precision_5:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity(king, राजा) = -0.0519\n",
      "Cosine Similarity(queen, रानी) = -0.0623\n",
      "Cosine Similarity(apple, सेब) = 0.1291\n"
     ]
    }
   ],
   "source": [
    "def cosine_sim(word1, word2):\n",
    "    return cosine_similarity([en_vocab[word1]], [hi_vocab[word2]])[0, 0]\n",
    "\n",
    "# Example word pairs\n",
    "example_pairs = [('king', 'राजा'), ('queen', 'रानी'), ('apple', 'सेब')]\n",
    "\n",
    "for en_word, hi_word in example_pairs:\n",
    "    if en_word in en_vocab and hi_word in hi_vocab:\n",
    "        sim = cosine_sim(en_word, hi_word)\n",
    "        print(f\"Cosine Similarity({en_word}, {hi_word}) = {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bilingual Lexicon Size: 5000, Precision@1: 0.0000, Precision@5: 0.0000\n",
      "Bilingual Lexicon Size: 10000, Precision@1: 0.0000, Precision@5: 0.0000\n",
      "Bilingual Lexicon Size: 20000, Precision@1: 0.0000, Precision@5: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for size in [5000, 10000, 20000]:\n",
    "    subset_dict = bilingual_dict[:size]\n",
    "    X_sub = np.array([en_vocab[pair[0]] for pair in subset_dict])\n",
    "    Y_sub = np.array([hi_vocab[pair[1]] for pair in subset_dict])\n",
    "    \n",
    "    W_sub = procrustes_alignment(X_sub, Y_sub)\n",
    "    X_aligned_sub = X_sub @ W_sub\n",
    "    p1, p5 = evaluate_translation(X_aligned_sub, subset_dict)\n",
    "    \n",
    "    print(f\"Bilingual Lexicon Size: {size}, Precision@1: {p1:.4f}, Precision@5: {p5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
