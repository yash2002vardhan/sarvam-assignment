{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, max_vocab=100000):\n",
    "    word2id, id2word, vectors = {}, {}, []\n",
    "    with open(file_path, 'r', encoding='utf-8', newline='\\n') as f:\n",
    "        next(f)  # Skip header\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx >= max_vocab:\n",
    "                break\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) < 10:\n",
    "                continue\n",
    "            try:\n",
    "                word, vec = tokens[0], np.array(tokens[1:], dtype=np.float32)\n",
    "            except:\n",
    "                continue\n",
    "            if vec.shape[0] != 300:\n",
    "                continue\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            vectors.append(vec)\n",
    "    return torch.tensor(np.vstack(vectors), dtype=torch.float32).to(device), word2id, id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(emb):\n",
    "    emb = emb - emb.mean(dim=0, keepdim=True)\n",
    "    emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(embedding_dim, embedding_dim))\n",
    "        nn.init.orthogonal_(self.W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.W\n",
    "\n",
    "    def orthogonalize(self, beta=0.01):\n",
    "        with torch.no_grad():\n",
    "            W = self.W.data\n",
    "            self.W.data = (1 + beta) * W - beta * W @ W.T @ W\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(4096, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csls_similarity(src_vecs, tgt_vecs, k=10):\n",
    "    src_vecs = F.normalize(src_vecs, dim=1, p=2)\n",
    "    tgt_vecs = F.normalize(tgt_vecs, dim=1, p=2)\n",
    "\n",
    "    sim_matrix = torch.matmul(src_vecs, tgt_vecs.T)\n",
    "    k = min(k, src_vecs.size(0), tgt_vecs.size(0))\n",
    "    src_knn_sim = torch.topk(sim_matrix, k, dim=1, largest=True).values.mean(dim=1)\n",
    "    tgt_knn_sim = torch.topk(sim_matrix, k, dim=0, largest=True).values.mean(dim=0)\n",
    "\n",
    "    csls_scores = 2 * sim_matrix - src_knn_sim[:, None] - tgt_knn_sim[None, :]\n",
    "    return csls_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, src_emb, tgt_emb, epochs=10, batch_size=128, lr=0.001, save_path=\"best_model.pth\", patience=5):\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    dis_opt = torch.optim.Adam(discriminator.parameters(), lr=lr * 2)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        generator.train()\n",
    "        discriminator.train()\n",
    "        epoch_loss_gen = 0\n",
    "        epoch_loss_dis = 0\n",
    "\n",
    "        for i in range(0, min(len(src_emb), len(tgt_emb)), batch_size):\n",
    "            src_batch = src_emb[i:i+batch_size]\n",
    "            tgt_batch = tgt_emb[i:i+batch_size]\n",
    "\n",
    "            # ----- Train Discriminator -----\n",
    "            for _ in range(5):\n",
    "                gen_emb = generator(src_batch).detach()\n",
    "                real = tgt_batch\n",
    "                fake = gen_emb\n",
    "\n",
    "                real_labels = torch.ones(real.size(0), 1).to(device)\n",
    "                fake_labels = torch.zeros(fake.size(0), 1).to(device)\n",
    "\n",
    "                dis_real = discriminator(real)\n",
    "                dis_fake = discriminator(fake)\n",
    "\n",
    "                loss_real = bce_loss(dis_real, real_labels)\n",
    "                loss_fake = bce_loss(dis_fake, fake_labels)\n",
    "                loss_dis = (loss_real + loss_fake) / 2\n",
    "\n",
    "                dis_opt.zero_grad()\n",
    "                loss_dis.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "            # ----- Train Generator -----\n",
    "            gen_emb = generator(src_batch)\n",
    "            pred = discriminator(gen_emb)\n",
    "            loss_gen = bce_loss(pred, torch.ones_like(pred))\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            gen_opt.step()\n",
    "            generator.orthogonalize()\n",
    "\n",
    "            epoch_loss_gen += loss_gen.item()\n",
    "            epoch_loss_dis += loss_dis.item()\n",
    "\n",
    "        avg_loss_gen = epoch_loss_gen / (len(src_emb) // batch_size)\n",
    "        avg_loss_dis = epoch_loss_dis / (len(src_emb) // batch_size)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Gen Loss: {avg_loss_gen:.4f}, Dis Loss: {avg_loss_dis:.4f}\")\n",
    "\n",
    "        # ----- Early stopping & saving -----\n",
    "        if avg_loss_gen < best_loss:\n",
    "            best_loss = avg_loss_gen\n",
    "            no_improve_epochs = 0\n",
    "            print(f\"‚ú® New best loss! Saving model to {save_path}\")\n",
    "            torch.save({\n",
    "                'generator': generator.state_dict(),\n",
    "                'discriminator': discriminator.state_dict(),\n",
    "            }, save_path)\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "            if no_improve_epochs >= patience:\n",
    "                print(f\"‚èπ Early stopping at epoch {epoch+1} due to no improvement for {patience} epochs.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(word, src_emb, tgt_emb, src_w2id, tgt_id2w, generator, top_k=1):\n",
    "    idx = src_w2id.get(word, None)\n",
    "    if idx is None:\n",
    "        return [\"<UNK>\"]\n",
    "\n",
    "    src_vec = src_emb[idx].unsqueeze(0)\n",
    "    projected = generator(src_vec)\n",
    "    csls_scores = compute_csls_similarity(projected, tgt_emb, k=10)\n",
    "    best_match_ids = csls_scores[0].topk(top_k).indices.tolist()\n",
    "    return [tgt_id2w[i] for i in best_match_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_words, hi_words  = [], []\n",
    "with open(\"en-hi-test.txt\", \"r\", encoding = \"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 2:\n",
    "            continue\n",
    "        en_word, hi_word = parts\n",
    "        en_words.append(en_word)\n",
    "        hi_words.append(hi_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Gen Loss: 7.6607, Dis Loss: 0.0332\n",
      "‚ú® New best loss! Saving model to best_model.pth\n",
      "Epoch 2: Gen Loss: 9.2987, Dis Loss: 0.0201\n",
      "Epoch 3: Gen Loss: 9.8930, Dis Loss: 0.0171\n",
      "Epoch 4: Gen Loss: 10.4937, Dis Loss: 0.0147\n",
      "Epoch 5: Gen Loss: 11.0700, Dis Loss: 0.0129\n",
      "Epoch 6: Gen Loss: 11.2328, Dis Loss: 0.0126\n",
      "‚èπ Early stopping at epoch 6 due to no improvement for 5 epochs.\n",
      "üîÑ Loading best saved model...\n",
      "‚úÖ Final Accuracy: 0.0000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    en_vecs, en_w2id, en_id2w = load_embeddings(\"wiki.en.vec\", max_vocab=100000)\n",
    "    hi_vecs, hi_w2id, hi_id2w = load_embeddings(\"wiki.hi.vec\", max_vocab=100000)\n",
    "\n",
    "    generator = Generator(300).to(device)\n",
    "    discriminator = Discriminator(300).to(device)\n",
    "\n",
    "    train(generator, discriminator, en_vecs, hi_vecs, epochs=50, batch_size=32, lr=0.0001, save_path=\"best_model.pth\", patience=5)\n",
    "\n",
    "    # Load best model\n",
    "    print(\"üîÑ Loading best saved model...\")\n",
    "    checkpoint = torch.load(\"best_model.pth\", map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator'])\n",
    "\n",
    "    # Evaluate\n",
    "    translated_words = []\n",
    "    for word in en_words:\n",
    "        translated = translate_word(word, en_vecs, hi_vecs, en_w2id, hi_id2w, generator)\n",
    "        translated_words.append(translated[0])\n",
    "\n",
    "    correct = 0\n",
    "    for pred, true in zip(translated_words, hi_words):\n",
    "        if pred == true:\n",
    "            correct += 1\n",
    "    score = correct / len(translated_words)\n",
    "    print(f\"‚úÖ Final Accuracy: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
