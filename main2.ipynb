{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(file_path, max_vocab=100000, expected_dim=300):\n",
    "    \"\"\"Loads FastText embeddings from a file, ignoring malformed lines.\"\"\"\n",
    "    word2id, id2word, vectors = {}, {}, []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)  # Skip header\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx >= max_vocab:\n",
    "                break\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) != expected_dim + 1:  # 1 for the word, rest for vector\n",
    "                continue  # skip bad lines\n",
    "            word = tokens[0]\n",
    "            vec = np.array(tokens[1:], dtype=np.float32)\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            vectors.append(vec)\n",
    "    \n",
    "    embedding_tensor = torch.tensor(np.vstack(vectors), dtype=torch.float32).to(device)\n",
    "    return embedding_tensor, word2id, id2word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.eye(dim).to(device))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.W\n",
    "\n",
    "    def orthogonalize(self):\n",
    "        with torch.no_grad():\n",
    "            W_np = self.W.data.cpu().numpy()\n",
    "            u, _, vt = np.linalg.svd(W_np)\n",
    "            self.W.data.copy_(torch.from_numpy(u @ vt).to(self.W.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, 512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 4096),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(4096, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_csls_similarity(src_vecs, tgt_vecs, k=5):\n",
    "    src_vecs = F.normalize(src_vecs, dim=1, p=2)\n",
    "    tgt_vecs = F.normalize(tgt_vecs, dim=1, p=2)\n",
    "\n",
    "    sim_matrix = torch.matmul(src_vecs, tgt_vecs.T)\n",
    "\n",
    "    k = min(k, tgt_vecs.shape[0], src_vecs.shape[0])  # make sure k isn't too big\n",
    "\n",
    "    src_knn_sim = torch.topk(sim_matrix, k, dim=1, largest=True).values.mean(dim=1)\n",
    "    tgt_knn_sim = torch.topk(sim_matrix, k, dim=0, largest=True).values.mean(dim=0)\n",
    "\n",
    "    csls_scores = 2 * sim_matrix - src_knn_sim[:, None] - tgt_knn_sim[None, :]\n",
    "    return csls_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_word(word, en_vecs, hi_vecs, en_w2id, hi_id2w, generator, top_k=1):\n",
    "    if word not in en_w2id:\n",
    "        return [\"<UNK>\"]\n",
    "    with torch.no_grad():\n",
    "        idx = en_w2id[word]\n",
    "        src_vec = en_vecs[idx].unsqueeze(0)\n",
    "        projected = generator(src_vec)\n",
    "        csls_scores = compute_csls_similarity(projected, hi_vecs, k=10)\n",
    "        best_match_ids = csls_scores[0].topk(top_k).indices.tolist()\n",
    "        return [hi_id2w[i] for i in best_match_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial(en_vecs, hi_vecs, generator, discriminator, epochs = 100, batch_size = 128, lr = 0.0005):\n",
    "\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr = lr)\n",
    "    dis_opt = torch.optim.Adam(discriminator.parameters(), lr = lr)\n",
    "    bce_loss = nn.BCELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(100):\n",
    "\n",
    "            en_idx = torch.randint(0, en_vecs.shape[0], (batch_size,))\n",
    "            hi_idx = torch.randint(0, hi_vecs.shape[0], (batch_size,))\n",
    "            x_en = en_vecs[en_idx]\n",
    "            x_hi = hi_vecs[hi_idx]\n",
    "\n",
    "\n",
    "            x_gen = generator(x_en).detach()\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "            dis_real = discriminator(x_hi)\n",
    "            dis_fake = discriminator(x_gen)\n",
    "\n",
    "            loss_real = bce_loss(dis_real, real_labels)\n",
    "            loss_fake = bce_loss(dis_fake, fake_labels)\n",
    "\n",
    "            dis_loss = loss_real + loss_fake\n",
    "            dis_opt.zero_grad()\n",
    "            dis_loss.backward()\n",
    "            dis_opt.step()\n",
    "\n",
    "            x_gen = generator(x_en)\n",
    "            fool_labels = torch.ones(batch_size, 1).to(device)\n",
    "            gen_loss = bce_loss(discriminator(x_gen), fool_labels)\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            generator.orthogonalize()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Dis Loss: {dis_loss.item():.4f} | Gen Loss: {gen_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Dis Loss: 0.4733 | Gen Loss: 4.9549\n",
      "Epoch 2/10 - Dis Loss: 0.4222 | Gen Loss: 4.4872\n",
      "Epoch 3/10 - Dis Loss: 0.1841 | Gen Loss: 7.3492\n",
      "Epoch 4/10 - Dis Loss: 0.1369 | Gen Loss: 7.3759\n",
      "Epoch 5/10 - Dis Loss: 0.1471 | Gen Loss: 7.8462\n",
      "Epoch 6/10 - Dis Loss: 0.1830 | Gen Loss: 8.4681\n",
      "Epoch 7/10 - Dis Loss: 0.1605 | Gen Loss: 7.1727\n",
      "Epoch 8/10 - Dis Loss: 0.1459 | Gen Loss: 7.7234\n",
      "Epoch 9/10 - Dis Loss: 0.1592 | Gen Loss: 7.9330\n",
      "Epoch 10/10 - Dis Loss: 0.1195 | Gen Loss: 6.9259\n",
      "EN → HI\n",
      "dog → टाइपसेटिंग\n",
      "king → बकरपुर\n",
      "water → विशय\n",
      "school → राजपत्रित\n",
      "city → नेवला\n"
     ]
    }
   ],
   "source": [
    "# Load your embeddings\n",
    "en_vecs, en_w2id, en_id2w = load_embeddings(\"wiki.en.vec\", max_vocab=100000)\n",
    "hi_vecs, hi_w2id, hi_id2w = load_embeddings(\"wiki.hi.vec\", max_vocab=100000)\n",
    "\n",
    "# Initialize models\n",
    "dim = en_vecs.shape[1]\n",
    "generator = Generator(dim).to(device)\n",
    "discriminator = Discriminator(dim).to(device)\n",
    "\n",
    "# Train\n",
    "train_adversarial(en_vecs, hi_vecs, generator, discriminator, epochs=10)\n",
    "\n",
    "# Test\n",
    "print(\"EN → HI\")\n",
    "for word in [\"dog\", \"king\", \"water\", \"school\", \"city\"]:\n",
    "    print(f\"{word} → {translate_word(word, en_vecs, hi_vecs, en_w2id, hi_id2w, generator)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Gen Loss: 9.2581, Dis Loss: 0.0130\n",
      "Epoch 2: Gen Loss: 10.9131, Dis Loss: 0.0028\n",
      "Epoch 3: Gen Loss: 11.0009, Dis Loss: 0.0034\n",
      "Epoch 4: Gen Loss: 12.7152, Dis Loss: 0.0036\n",
      "Epoch 5: Gen Loss: 12.1210, Dis Loss: 0.0017\n",
      "Epoch 6: Gen Loss: 12.0103, Dis Loss: 0.0012\n",
      "Epoch 7: Gen Loss: 12.5229, Dis Loss: 0.0047\n",
      "Epoch 8: Gen Loss: 13.5786, Dis Loss: 0.0011\n",
      "Epoch 9: Gen Loss: 12.3497, Dis Loss: 0.0008\n",
      "Epoch 10: Gen Loss: 12.4586, Dis Loss: 0.0005\n",
      "Epoch 11: Gen Loss: 11.1901, Dis Loss: 0.0013\n",
      "Epoch 12: Gen Loss: 11.0986, Dis Loss: 0.0016\n",
      "Epoch 13: Gen Loss: 12.4666, Dis Loss: 0.0008\n",
      "Epoch 14: Gen Loss: 13.1001, Dis Loss: 0.0012\n",
      "Epoch 15: Gen Loss: 11.4810, Dis Loss: 0.0005\n",
      "Epoch 16: Gen Loss: 11.8347, Dis Loss: 0.0021\n",
      "Epoch 17: Gen Loss: 14.3655, Dis Loss: 0.0001\n",
      "Epoch 18: Gen Loss: 13.9018, Dis Loss: 0.0008\n",
      "Epoch 19: Gen Loss: 11.9682, Dis Loss: 0.0011\n",
      "Epoch 20: Gen Loss: 13.5093, Dis Loss: 0.0041\n",
      "Epoch 21: Gen Loss: 15.2070, Dis Loss: 0.0035\n",
      "Epoch 22: Gen Loss: 12.8941, Dis Loss: 0.0005\n",
      "Epoch 23: Gen Loss: 14.1698, Dis Loss: 0.0010\n",
      "Epoch 24: Gen Loss: 15.4617, Dis Loss: 0.0014\n",
      "Epoch 25: Gen Loss: 15.2324, Dis Loss: 0.0003\n",
      "Epoch 26: Gen Loss: 14.8973, Dis Loss: 0.0001\n",
      "Epoch 27: Gen Loss: 16.0026, Dis Loss: 0.0014\n",
      "Epoch 28: Gen Loss: 15.7933, Dis Loss: 0.0002\n",
      "Epoch 29: Gen Loss: 14.2406, Dis Loss: 0.0108\n",
      "Epoch 30: Gen Loss: 14.4944, Dis Loss: 0.0016\n",
      "Epoch 31: Gen Loss: 14.4647, Dis Loss: 0.0062\n",
      "Epoch 32: Gen Loss: 15.4031, Dis Loss: 0.0006\n",
      "Epoch 33: Gen Loss: 14.7072, Dis Loss: 0.0003\n",
      "Epoch 34: Gen Loss: 16.0816, Dis Loss: 0.0005\n",
      "Epoch 35: Gen Loss: 16.2919, Dis Loss: 0.0002\n",
      "Epoch 36: Gen Loss: 15.5256, Dis Loss: 0.0002\n",
      "Epoch 37: Gen Loss: 17.6085, Dis Loss: 0.0020\n",
      "Epoch 38: Gen Loss: 19.3977, Dis Loss: 0.0061\n",
      "Epoch 39: Gen Loss: 16.9448, Dis Loss: 0.0017\n",
      "Epoch 40: Gen Loss: 18.8185, Dis Loss: 0.0002\n",
      "Epoch 41: Gen Loss: 18.2986, Dis Loss: 0.0003\n",
      "Epoch 42: Gen Loss: 18.8871, Dis Loss: 0.0002\n",
      "Epoch 43: Gen Loss: 17.5498, Dis Loss: 0.0022\n",
      "Epoch 44: Gen Loss: 15.6180, Dis Loss: 0.0001\n",
      "Epoch 45: Gen Loss: 16.4588, Dis Loss: 0.0002\n",
      "Epoch 46: Gen Loss: 18.2868, Dis Loss: 0.0000\n",
      "Epoch 47: Gen Loss: 15.6335, Dis Loss: 0.0046\n",
      "Epoch 48: Gen Loss: 18.1014, Dis Loss: 0.0000\n",
      "Epoch 49: Gen Loss: 18.3368, Dis Loss: 0.0009\n",
      "Epoch 50: Gen Loss: 13.3513, Dis Loss: 0.0018\n",
      "dog → लियो\n",
      "king → गले\n",
      "water → सान्याल\n",
      "school → पोकीमॉन\n",
      "city → विरासत\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Use MPS if available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------------- Load Embeddings ---------------------------\n",
    "def load_embeddings(file_path, max_vocab=20000):\n",
    "    word2id, id2word, vectors = {}, {}, []\n",
    "    with open(file_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)  # Skip header\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx >= max_vocab:\n",
    "                break\n",
    "            tokens = line.strip().split()\n",
    "            if len(tokens) < 10:\n",
    "                continue\n",
    "            word, vec = tokens[0], np.array(tokens[1:], dtype=np.float32)\n",
    "            if vec.shape[0] != 300:\n",
    "                continue\n",
    "            word2id[word] = idx\n",
    "            id2word[idx] = word\n",
    "            vectors.append(vec)\n",
    "    return torch.tensor(np.vstack(vectors), dtype=torch.float32).to(device), word2id, id2word\n",
    "\n",
    "# ------------------------- Normalize Embeddings ---------------------------\n",
    "def normalize_embeddings(emb):\n",
    "    emb = emb - emb.mean(dim=0, keepdim=True)\n",
    "    emb = F.normalize(emb, p=2, dim=1)\n",
    "    return emb\n",
    "\n",
    "# ------------------------- Generator ---------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(embedding_dim, embedding_dim))\n",
    "        nn.init.orthogonal_(self.W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x @ self.W\n",
    "\n",
    "    def orthogonalize(self, beta=0.01):\n",
    "        with torch.no_grad():\n",
    "            W = self.W.data\n",
    "            self.W.data = (1 + beta) * W - beta * W @ W.T @ W\n",
    "\n",
    "# ------------------------- Discriminator ---------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4096),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(4096, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# ------------------------- CSLS Similarity ---------------------------\n",
    "def compute_csls_similarity(src_vecs, tgt_vecs, k=10):\n",
    "    src_vecs = F.normalize(src_vecs, dim=1, p=2)\n",
    "    tgt_vecs = F.normalize(tgt_vecs, dim=1, p=2)\n",
    "\n",
    "    sim_matrix = torch.matmul(src_vecs, tgt_vecs.T)\n",
    "    k = min(k, src_vecs.size(0), tgt_vecs.size(0))\n",
    "    src_knn_sim = torch.topk(sim_matrix, k, dim=1, largest=True).values.mean(dim=1)\n",
    "    tgt_knn_sim = torch.topk(sim_matrix, k, dim=0, largest=True).values.mean(dim=0)\n",
    "\n",
    "    csls_scores = 2 * sim_matrix - src_knn_sim[:, None] - tgt_knn_sim[None, :]\n",
    "    return csls_scores\n",
    "\n",
    "# ------------------------- Training Loop ---------------------------\n",
    "def train(generator, discriminator, src_emb, tgt_emb, epochs=10, batch_size=128, lr=0.001):\n",
    "    gen_opt = torch.optim.Adam(generator.parameters(), lr=lr)\n",
    "    dis_opt = torch.optim.Adam(discriminator.parameters(), lr=lr * 2)\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, min(len(src_emb), len(tgt_emb)), batch_size):\n",
    "            src_batch = src_emb[i:i+batch_size]\n",
    "            tgt_batch = tgt_emb[i:i+batch_size]\n",
    "\n",
    "            # Discriminator\n",
    "            for _ in range(5):\n",
    "                gen_emb = generator(src_batch).detach()\n",
    "                real = tgt_batch\n",
    "                fake = gen_emb\n",
    "\n",
    "                real_labels = torch.ones(real.size(0), 1).to(device)\n",
    "                fake_labels = torch.zeros(fake.size(0), 1).to(device)\n",
    "\n",
    "                dis_real = discriminator(real)\n",
    "                dis_fake = discriminator(fake)\n",
    "\n",
    "                loss_real = bce_loss(dis_real, real_labels)\n",
    "                loss_fake = bce_loss(dis_fake, fake_labels)\n",
    "                loss_dis = (loss_real + loss_fake) / 2\n",
    "\n",
    "                dis_opt.zero_grad()\n",
    "                loss_dis.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "            # Generator\n",
    "            gen_emb = generator(src_batch)\n",
    "            pred = discriminator(gen_emb)\n",
    "            loss_gen = bce_loss(pred, torch.ones_like(pred))\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            loss_gen.backward()\n",
    "            gen_opt.step()\n",
    "            generator.orthogonalize()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Gen Loss: {loss_gen.item():.4f}, Dis Loss: {loss_dis.item():.4f}\")\n",
    "\n",
    "# ------------------------- Translate Word ---------------------------\n",
    "def translate_word(word, src_emb, tgt_emb, src_w2id, tgt_id2w, generator, top_k=1):\n",
    "    idx = src_w2id.get(word, None)\n",
    "    if idx is None:\n",
    "        return [\"<UNK>\"]\n",
    "\n",
    "    src_vec = src_emb[idx].unsqueeze(0)\n",
    "    projected = generator(src_vec)\n",
    "    csls_scores = compute_csls_similarity(projected, tgt_emb, k=10)\n",
    "    best_match_ids = csls_scores[0].topk(top_k).indices.tolist()\n",
    "    return [tgt_id2w[i] for i in best_match_ids]\n",
    "\n",
    "# ------------------------- Main Pipeline ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    en_vecs, en_w2id, en_id2w = load_embeddings(\"wiki.en.vec\", max_vocab=20000)\n",
    "    hi_vecs, hi_w2id, hi_id2w = load_embeddings(\"wiki.hi.vec\", max_vocab=20000)\n",
    "\n",
    "    en_vecs = normalize_embeddings(en_vecs)\n",
    "    hi_vecs = normalize_embeddings(hi_vecs)\n",
    "\n",
    "    generator = Generator(300).to(device)\n",
    "    discriminator = Discriminator(300).to(device)\n",
    "\n",
    "    train(generator, discriminator, en_vecs, hi_vecs, epochs=50, batch_size=32, lr=0.0001)\n",
    "\n",
    "    for word in [\"dog\", \"king\", \"water\", \"school\", \"city\"]:\n",
    "        translated = translate_word(word, en_vecs, hi_vecs, en_w2id, hi_id2w, generator)\n",
    "        print(f\"{word} → {translated[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
